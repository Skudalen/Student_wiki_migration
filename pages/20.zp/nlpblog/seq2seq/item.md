---
title: NeurÃ³novÃ© siete Sequence-to-sequence
published: true
taxonomy:
    category: [tp2020]
    tag: [nn,seq2seq,translation,nlp]
    author: Dominik Nagy
---
TÃ­movÃ½ projekt 2020, Dominik Nagy


HlbokÃ© neurÃ³novÃ© siete (Deep Neural Networks â€“ DNN) sÃº veÄ¾mi vÃ½konnÃ© modely strojovÃ©ho
uÄenia, ktorÃ© sÃº urÄenÃ© na rieÅ¡enie zloÅ¾itÃ½ch problÃ©mov, ako je rozpoznÃ¡vanie reÄi Äi vizuÃ¡lnych objekov.
HlbokÃ© neurÃ³novÃ© siete mÃ´Å¾u vykonÃ¡vaÅ¥ Ä¾ubobovoÄ¾nÃ½ paralelnÃ½ vÃ½poÄet na malÃ½ poÄet krokov. VeÄ¾kÃ©
DNN mÃ´Å¾u byÅ¥ trÃ©novanÃ© pokiaÄ¾ mÃ¡me dostupnÃ© obrovskÃ© mnoÅ¾stvo dÃ¡t. Napriek svojej flexibilite a sile,
hlbokÃ© neurÃ³novÃ© siete sa dajÃº aplikovaÅ¥ iba na problÃ©my, ktorÃ½ch vstupy mÃ´Å¾u byÅ¥ rozumne kÃ³dovanÃ©
vektormi s fixnÃ½m rozmerom. [1]

Sequence-to-sequence alebo postupnosÅ¥ sekvenciÃ­ je o trÃ©novanÃ­ modelov na konverziu z jednej
domÃ©ny na sekvencie do inej domÃ©ne ako naprÃ­klad preloÅ¾enie viet z angliÄtiny do slovenÄiny.
PostupnosÅ¥ sekvenciÃ­ mapuje jednu sekvenciu neurÄenej dÄºÅ¾ky na inÃº sekvenciu, ktorej dÄºÅ¾ka je tieÅ¾
neznÃ¡ma. [2]

RekurentnÃ¡ NeurÃ³novÃ¡ SieÅ¥ (po anglicky Recurrent Neural Network, Äalej uÅ¾ len RNN) je prirodenÃ¡
generalizÃ¡cia doprednÃ½ch neurÃ³novÃ½ch sietÃ­ pre sekvencie. PostupnosÅ¥ vstupu (x 1 ,...,xT), Å¡tandardnÃ© RNN
vypoÄÃ­ta postupnosÅ¥ vÃ½stupov (y 1 ,...,yT) iterÃ¡ciu nasledujÃºcich rovnÃ­c:

### â„<sub>ğ‘¡</sub> = ğ‘ ğ‘–ğ‘”ğ‘š(ğ‘Š<sup>â„ğ‘¥</sup>ğ‘¥<sub>ğ‘¡</sub> + ğ‘Š<sup>â„â„</sup>â„<sub>ğ‘¡âˆ’1</sub>)


### ğ‘¦<sub>ğ‘¡</sub> = ğ‘Š<sup>ğ‘¦â„</sup>â„<sub>t</sub>

RNN mÃ´Å¾e Ä¾ahko mapovaÅ¥ sekvencie na sekvencie vÅ¾dy, keÄ je zarovnanie medzi vstupmi vÃ½stupmi
znÃ¡me vopred. Nie je vÅ¡ak jasnÃ©, ako aplikovaÅ¥ RNN na problÃ©my, ktorÃ½ch vstupnÃ© a vÃ½stupnÃ© sekvencie
majÃº rÃ´zne dÄºÅ¾ky s komplikovanÃ½mi a nemonotonickÃ½mi vzÅ¥ahmi. NajjednoduchÅ¡ia stratÃ©gia pre
vÅ¡eobecnÃ© sekvenÄÅ„Ã© uÄenie je mapovaÅ¥ vstupnÃº sekvenciu na vektor s pevnou veÄ¾kosÅ¥ou pomocou
jednÃ©ho RNN a potom mapovaÅ¥ vektor na cieÄ¾ovÃº sekvenciu s inÃ½m RNN.[2] [3]

## Convolutional neural networks

KonvoluÄnÃ© neurÃ³novÃ© siete sÃº menej beÅ¾nÃ© pre sekvenÄnÃ© modelovanie, napriek niekoÄ¾kÃ½m vÃ½hodÃ¡m.
V porovnanÃ­ s opakujÃºcimi sa vrstvami, konvolÃ¡cie vytvÃ¡rajÃº reprezentÃ¡ciu pre kontexty s pevnou
veÄ¾kosÅ¥ou, avÅ¡ak efektÃ­vna veÄ¾kosÅ¥ kontextu siete sa dÃ¡ Ä¾ahko zvÃ¤ÄÅ¡iÅ¥ naskladanÃ­m niekoÄ¾kÃ½ch vrstiev na
seba. To umoÅ¾Åˆuje presne ovlÃ¡daÅ¥ maximÃ¡lnu dÄºÅ¾ku zÃ¡vislostÃ­, ktorÃ© sa majÃº modelovaÅ¥. KonvoluÄnÃ©
siete nezÃ¡visia od vÃ½poÄtov predchÃ¡dzajÃºceho ÄasovÃ©ho kroku, a preto umoÅ¾ÅˆujÃº paralelizÃ¡ciu nad
kaÅ¾dÃ½m prvkom v sekvencii. Tento kontrast v RNN, ktorÃ½ udrÅ¾iava skrytÃ½ stav celej minulosti, zabraÅˆuje
paralelnÃ©mu vÃ½poÄtu v danej sekvencii [4]


# Encoder-decoder

HlbokÃ© neurÃ³novÃ© siete preukÃ¡zali veÄ¾kÃ½ Ãºspech v rÃ´znych aplikÃ¡ciÃ¡ch, ako naprÃ­klad
rozpoznÃ¡vanie objektov alebo rozpoznÃ¡vanie reÄi. NedÃ¡vno sa objavil novÃ½ prÃ­stup k Å¡tatistickÃ©mu
strojovÃ©mu prekladu zaloÅ¾enÃ½ na neurÃ³novÃ½ch sieÅ¥ach. Tento novÃ½ prÃ­stup je inÅ¡pirovanÃ½ podÄ¾a
nedÃ¡vneho trendu hlbokÃ©ho reprezentatÃ­vneho uÄenia. VÅ¡etky modely neurÃ³novej siete pouÅ¾itÃ© v [5]
pozostÃ¡vajÃº z kÃ³dera a dekÃ³dera (encoder, decoder) KÃ³der extrahuje vektor s pevnou dÄºÅ¾kou
reprezentujÃºce z vety s premenlivou dÄºÅ¾kou a z tohto znÃ¡zornenia dekÃ³der generuje sprÃ¡vny cieÄ¾ovÃ½
preklad s premenlivou dÄºÅ¾kou. Model neurÃ³novÃ©ho strojovÃ©ho prekladu vyÅ¾aduje iba zlomok pamÃ¤te,
ktorÃº potrebuje model tradiÄnÃ©ho strojovÃ©ho prekladu. [6][7]


 ![Encoder-decoder](encoder_decoder.png)
 
1 IlustrÃ¡cia RNN Encoder-Decoder [5]

# Transformer a Attention

OpakujÃºce sa seq2seq modely, ktorÃ© pouÅ¾Ã­vajÃº encoder-decoder architektÃºru dosiahli veÄ¾kÃ½
pokrok v rozpoznÃ¡vanÃ­ reÄi. AvÅ¡ak, majÃº nevÃ½hodu v rÃ½chlosti trÃ©ningu. VnÃºtornÃ© opakovanie obmedzuje
pararelilÃ¡ziu trÃ©ningu. NeopakujÃºci sa seq2seq model nazÃ½vanÃ½ Transformer sa spolieha na mechanizmy
Attention, aby sa nauÄil poziÄnÃ½m zÃ¡vislostiam, ktorÃ© je moÅ¾nÃ© trÃ©novaÅ¥ rÃ½chlejÅ¡ie s vÃ¤ÄÅ¡ou
ÃºÄinnosÅ¥ou.[8] Attention model sa nesnaÅ¾Ã­ preloÅ¾iÅ¥ vetu naraz, preklad prebieha postupne, preloÅ¾Ã­ najprv
jednu ÄasÅ¥ vety a potom pokraÄuje na ÄalÅ¡iu casÅ¥, ako Älovek.

Attention model by sa dal vysvetliÅ¥ ako funckiu, ktorÃ¡ mapuje dopyt a sadu pÃ¡ru kÄ¾ÃºÄ-hodnota na vÃ½stup,
kde dopyt, hodnoty, kÄ¾ÃºÄe a vÃ½stup sÃº vektory. VÃ½stup sa poÄÃ­ta ako vÃ¡Å¾enÃ¡ suma (weighted sum)
hodnÃ´t, kde vÃ¡ha priradenÃ¡ kaÅ¾dej hodnote sa vypoÄÃ­ta poÄla funcie kompatibility dopytu so
zodpovedajÃºcim kÄ¾ÃºÄom.[9]


![Scaled-Dot_Multi-Head](scaled-multi.png)

2 Scaled Dot-Product Attention a Multi-Head attention

## Scaled Dot-Product Attention

Vstup pozostÃ¡va z dopytov a kÄ¾ÃºÄov rozmeru _dk_ a hodnÃ´t rozmerov _dv._ VypoÄÃ­tajÃº sa â€dot productsâ€œ
vÅ¡etkÃ½ch dopytov s kÄ¾ÃºÄmi, vydelia sa s _âˆšdk_ a pouÅ¾ije sa funkcia â€softmaxâ€ na zÃ­skane vÃ¡hy hodnÃ´t. [9]

![Attention](attention.png)

## Multi-Head Attention

Silnou strÃ¡nkou Multi-Head Attentionu je schopnosÅ¥ spoloÄne sa venovaÅ¥ informÃ¡ciÃ¡m z rÃ´znych
reprezentaÄnÃ½ch podpriestorov na rÃ´znych pozÃ­ciÃ¡ch.[10] [9]


### ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ‘„,ğ¾, ğ‘‰) = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘<sub>1</sub>, â€¦ , â„ğ‘’ğ‘ğ‘‘<sub>â„</sub>)ğ‘Š<sup>O</sup>

### ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ â„ğ‘’ğ‘ğ‘‘<sub>ğ‘–</sub> = ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„ğ‘Š<sub>ğ‘–</sub><sup>ğ‘„</sup>,ğ¾ğ‘Š<sub>ğ‘–</sub><sup>K</sup>,ğ‘‰ğ‘Š<sub>ğ‘–</sub><sup>V</sup>)

# Zoznam pouÅ¾itej literatÃºry

[1] I. Sutskever Google, O. Vinyals Google, and Q. V Le Google, â€œSequence to Sequence Learning with
Neural Networks.â€

[2] M. P. For, â€œNatural Language Processing in Action,â€ _Online_ , vol. 80, no. 1. p. 453, 2017.

[3] â€œSequence to Sequence Learning with Neural Networks â€“ arXiv Vanity.â€ [Online]. Available:
https://www.arxiv-vanity.com/papers/1409.3215/. [Accessed: 20-Dec-2019].

[4] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, â€œConvolutional Sequence to
Sequence Learning.â€

[5] K. Cho _et al._ , â€œLearning phrase representations using RNN encoder-decoder for statistical
machine translation,â€ in _EMNLP 2014 - 2014 Conference on Empirical Methods in Natural
Language Processing, Proceedings of the Conference_ , 2014, pp. 1724â€“1734.

[6] R. Pascanu, T. Mikolov, and Y. Bengio, â€œOn the difficulty of training recurrent neural networks,â€ in
_30th International Conference on Machine Learning, ICML 2013_ , 2013, no. PART 3, pp. 2347â€“
2355.

[7] K. Cho, B. Van MerriÃ«nboer, D. Bahdanau, and Y. Bengio, â€œOn the Properties of Neural Machine
Translation: Encoder-Decoder Approaches.â€

[8] L. Dong, S. Xu, and B. Xu, _SPEECH-TRANSFORMER: A NO-RECURRENCE SEQUENCE-TO-SEQUENCE
MODEL FOR SPEECH RECOGNITION_..

[9] A. Vaswani _et al._ , â€œAttention Is All You Need.â€

[10] J. Li, Z. Tu, B. Yang, M. R. Lyu, and T. Zhang, â€œMulti-Head Attention with Disagreement
Regularization.â€


