---
title: Crowdsourcing
published: true
taxonomy:
    category: [tp2020]
    tag: [annotation,nlp]
    author: Jakub Maruniak
---
Tímový projekt 2019, Jakub Maruniak


Čo je to crowdsourcing? Výraz _crowdsourcing_ bol prvý krát použitý v júni 2006, kedy editor magazínu Wired, Jeff Howe, vydal článok „The Rise of Crowdsourcing&quot; [1]. V tomto článku a v ďalších príspevkoch na svojom blogu popisuje novú organizačnú formu, koncept, pri ktorom je problém zadaný neznámej skupine riešiteľov. Zákazníci, alebo žiadatelia môžu uverejniť požadované úlohy na crowdsourcingovú platformu, kde dodávatelia – skupina, alebo jednotlivci vykonajú tieto úlohy na základe ich záujmov a schopností [2].

Podobne ako mnoho iných výrazov, ktoré boli po prvý krát použité v magazíne Wired, aj výraz crowdsourcing sa rýchlo udomácnil a začal byť používaný širokou verejnosťou. Podľa vyjadrenia Howa, sa vyhľadávanie termínu crowdsourcing pomocou Googlu zvýšilo v priebehu jedného týždňa po uverejnení článku na jeho blogu z 3 až na 180 000 výsledkov. Táto téma je aj dnes veľmi diskutovanou oblasťou výskumu, pričom v priebehu posledných 5 rokov bolo publikovaných viac ako 10 000 publikácii [3]. Platformy, akými sú Amazon Mechanical Turk, alebo CrowdFlower poskytujú možnosť jednoduchého spojenia zákazníkov a dodávateľov.

Presne definovať crowdsourcing je kvôli rozdielnemu pohľadu na túto problematiku takmer nemožné. To, čo niektorí autori považujú za súčasť tejto oblasti nemusia iní odborníci klasifikovať ako crowdsourcing. Príkladom môžu byť tvrdenia autora Buechelera [4], ktorý vo svojom výskume opisuje Wikipediu ako príklad crowdsourcingu, alebo autor Huberman [5] na tento príklad využíva YouTube, zatiaľ čo Frank Kleeman [6] vo svojom článku tvrdí úplný opak. Množstvo rozdielnych definícii má za následok to, že crowdsourcing nemožno definovať koherentne. Estellés-Arolas a Fernando Gonzáles-Ladrón-de-Guevara vydali v roku 2012 odborný článok venujúci sa tejto problematike, pričom zozbierali odbornú literatúru na túto tému a našli takmer 40 rozdielnych interpretácií tohto termínu s viacerými konfliktmi [7]. Aj podľa vyjadrenia Darena Brabhama sa začalo mnoho médií využívať ako príklad pre crowdsourcing, avšak nemajú nič spoločné s ich pôvodnou štruktúrou. Spomenul napríklad Wikipediu, YouTube, Flickr, blogy a open-source softvéry. Všetko, kde sa nachádzala väčšia skupina ľudí, ktorá robila čokoľvek bola chybne považovaná za crowdsourcing, pričom aj v mnohých známych médiách bola o tejto problematike rozširovaná mätúca informácia o tom, čo to vlastne je [1]. Estellés-Arolas a Fernando Guaevara vo svojej práci rozanalyzovali dostupné informácie a dospeli k záveru, že správna definícia by mala byť: „Crowdsourcing je typ online participatívnej aktivity, v ktorej jednotlivec, inštitúcia, nezisková organizácia, alebo spoločnosť navrhne skupine jednotlivcov s rôznymi znalosťami, rôzneho pohlavia a počtu prostredníctvom verejnej výzvy možnosť dobrovoľného vykonávania úlohy. Vykonanie úlohy variabilnej zložitosti a modularity, ktorej sa skupina zúčastní prínosom vo forme svojej práci, peňazí, vedomostí a/alebo skúseností prináša vždy vzájomný prospech. Používateľ je uspokojený daným typom potreby, či už je to finančným spôsobom, sociálnym uznaním, sebauspokojením alebo rozvojom individuálnych zručností, zatiaľ čo zadávateľ získa a využije vo svoj prospech to, čo používateľ doniesol do projektu, ktorého forma závisí od typu vykonávanej činnosti. &quot; [7]

Využitie tohto konceptu so sebou prináša aj potenciálne benefity, medzi ktoré môžeme zaradiť: [2], [8]

- Náklady – Odmeny za vykonanú prácu sa značne líšia na základe typu vykonávanej práce, začínajú na mikro platbách, ale môžu sa vyšplhať až na niekoľko stovák, resp. tisícok eur. Zákazníci platia iba za vedomosti, ktoré potrebujú pre svoj konkrétny projekt. Účastníci crowdsourcingových projektov sú často krát amatéri, čerství absolventi univerzít alebo jednotlivci, ktorí si chcú vo voľnom čase privyrobiť a teda nemajú také vysoké nároky ako profesionáli. Taktiež niektoré typy úloh sú vykonávane zadarmo, na báze dobrovoľníctva.
- Kvalita – vďaka prístupu k širokej skupine vývojárov, ktorí majú požadovanú zručnosť k vypracovaniu daného problému si má zadávateľ možnosť vybrať riešenie, ktoré najviac vyhovuje jeho podmienkam, resp. má prístup k väčšiemu množstvu dosiahnutých výsledkov.
- Rýchlosť – vďaka prístupu k väčšiemu množstvu talentov je možné vyvíjať, resp. zadať niekoľko úloh súčasne, nakoľko úlohy sa neviažu ku konkrétnemu vývojárovi. Taktiež sú tvorcovia na crowdsourcingových platformách často ochotní pracovať vo svojom voľnom čase, alebo cez víkendy, takže požadovaná úloha môže byť splnená o výrazne nižší čas ako v porovnaní s klasickými zamestnancami vo firmách.
- Kreativita a inovácie – rozmanitosť odborných znalostí, ktoré poskytuje dav je omnoho väčšia ako v porovnaní s väčšinou jednotlivých spoločností. To zaisťuje, že riešenia môžu byť kreatívnejšie.

Na základe týchto benefitov možno konštatovať a dedukovať, že čoraz viac úloh bude riešených práve takouto formou. Aj Lakhani a Panetta v ich štúdii poukazovali na to, že splniť úlohy požadujúce konkrétne znalosti bude pomocou klasického uzatvoreného modelu čoraz náročnejšie, nakoľko väčšina potrebných znalostí sa bude nachádzať mimo túto organizáciu [2], [9].





**Crowdsourcing a spracovanie prirodzeného jazyka**

 _Natural Language Processing (NLP),_ alebo spracovanie prirodzeného jazyka je oblasť výskumu, ktorá prepája lingvistiku a umelú inteligenciu. Prirodzený jazyk je v porovnaní s počítačovými programovacími jazykmi úplne odlišný. Prirodzený jazyk slúži na komunikáciu medzi ľuďmi a vzájomné vymieňanie si informácii. Nie sú určené na preklad do konečného cyklu matematických operácii, aké používajú programovacie jazyky. Počítačový program napísaný v určitom programovacom jazyku odovzdá stroju konkrétnu úlohu, ktorú má vykonať. Neexistuje však kompilátor pre prirodzený jazyk, akými sú napríklad slovenčina, alebo angličtina.

Spracovanie prirodzeného jazyka predstavuje v princípe preklad prirodzeného jazyka na dáta (čísla), pomocou ktorých sa počítač učí o svete.  Toto chápanie sveta môže byť následne použité na generovanie textu v prirodzenom jazyku, ktorý odráža toto chápanie [10].

Systém spracovania prirodzeného jazyka je často nazývaný „pipeline&quot;, teda potrubie. Toto pomenovanie môžeme chápať ako zreťazené spracovanie, ktoré pozostáva z niekoľkých častí, pričom vstupom na začiatku je prirodzený jazyk a na konci je spracovaný výstup. Myšlienkou je rozdeliť problém na malé kúsky a následne využiť strojové učenie na vyriešenie každého z týchto kúskov osobitne. Zreťazením viacerých modelov strojového učenia tak môžeme spracovať aj komplikovanejšie vstupy [11]. Je známym faktom, že výkonnosť systémov strojového učenia vo veľkej miere závisí aj od množstva a kvality dát, ktoré slúžia ako vstup pre tréningový algoritmus.

V priebehu posledných 10 rokoch bolo spracovanie prirodzeného jazyka posúvané vpred vďaka stále rastúcemu množstvu anotovaných dát – korpusov. Tie sú potrebné na trénovanie NLP algoritmov, ich vyhodnocovanie, ako aj umožnenie porovnávania algoritmov a opakované experimentovanie. V minulosti sa o produkovanie takýchto dát starali projekty ako napríklad ACE-2004, TAC, SemEval a Senseval alebo veľký projekt OntoNotes [12]–[15]. Tie sa ukázali, ako základ pre tréning NLP algoritmu a vďaka nim dnes vieme, aký spôsob je najlepší pre vytváranie anotácii vysokej kvality prostredníctvom zamestnávania, školenia a riadenia skupín lingvistických expertov. V posledných rokoch vznik rôznych crowdsourcových platforiem podmienil výskumníkov v oblasti NLP, aby experimentovali s týmto modelom ako novým prístupom pri získavaní jazykových korpusov [16]. Mnoho pozornosti si tieto platformy získali aj vďaka tomu, že získavanie anotácii pre tréning systémov umelej inteligencie pomocou klasických metód bolo často namáhavé a veľmi nákladné. Viacero výskumov na túto tému v poslednej dobe poukazuje na fakt, že pokiaľ sú anotácie získané od bežných ľudí v dostatočnej kvantite, dokážu sa svojou kvalitou vyrovnať, alebo často krát aj prekonať anotácie od expertov a to za podstatne nižšie náklady [17], [18], [19].

Vďaka crowdsourcingu majú výskumníci jednoduchý prístup k veľkej zásobe lacnej práce a poskytuje im potenciál na podrobné skúmanie jazykových javov vďaka obrovskému množstvu vzoriek ľudských anotovaných dát. Táto metóda získavania dát je veľmi cenná pre počítačových lingvistov ako zdroj označených tréningových dát na využitie v strojovom učení a taktiež ako prostriedok na zhromažďovanie údajov zo spoločenských vied, ktoré spájajú používanie jazyka so základným presvedčením a správaním [3].

Efektívne zbieranie crowdsourcových dát vyžaduje pozorné kladenie dôrazu na samotný proces, od výberu vhodnej skupiny pracovníkov, cez zadanie jasných inštrukcii, ktoré by mali byť pochopiteľné pre každého, až po vykonanie kontroly zozbieraných dát a eliminovanie spamu, ktorý bol vyplnený náhodne alebo zlým spôsobom, iba za účelom získania odmeny.

Model crowdsourcingu pre vytváranie korpusu môže mať 3 formy. Prvou formou je mechanizovaná práca, kde sú pracovníci zvyčajne finančne odmenení. Sem patria problémy, ktoré sú subjektívne a zatiaľ nemôžu byť spoľahlivo riešené automaticky, ako napríklad získavanie názorov a citov (Mellebeek a kol., 2010) [19], rozpoznanie správneho výrazu slov (Parent, Eskenazi, 2010) [20], odpovedanie na otázky (Heilman a Smith, 2010) [21], korpusy získavané zo špeciálnych zdrojov – Twitter (Finin a kol., 2010) [22], alebo emailová komunikácia (Lawson a kol., 2010) [23] a mnoho ďalších. Nespornou výhodou crowdsourcingu je prístup k zahraničným trhom a k ľuďom, ktorých materinský jazyk je nezvyčajný, exotický. Ďalšou formou crowdsourcingu sú hry s určitým účelom (Games with a purpose – GWAP), v tomto prípade na anotáciu textov.  Jedná sa o alternatívnu metódu anotačnej metódy za využitia hry. Úloha je pretransformovaná na hru, výsledkom ktorej sú anotačné dáta. Podľa niektorých výskumov vedie využitie tejto metódy ku kvalitnejším výsledkom a väčšiemu nasadeniu anotátorov vďaka tomu, že ku práci je pridaná zábavná zložka. Taktiež môže hra osloviť iné publikum, než iba klasickú skupinu crowdsourcerov, a teda môže poskytnúť väčšie množstvo výsledkov [24]. Väčšina takýchto hier v sebe obsahuje prvky hier, akými sú počítadlo skóre, výber obtiažnosti, tabuľku najlepších hráčov, ale samotná hra je z väčšej bázy v textovej forme so silnou podobnosťou s tradičnou anotáciou. V niektorých výskumoch sa však vývojári snažili o opačný prístup a navrhli klasickú grafickú hru, do ktorej následne importovali prvky anotácie. Príkladom pre takúto hru môže byť napríklad _Puzzle Racer_ [24]. Poslednou formou je využívanie dát od dobrovoľníkov, ktorí sa rozhodli určitým spôsobom prispieť k anotácii. Príkladom môže byť určovanie vhodného významu konkrétneho slova vo vete, určovanie typov entít vo vetách, hľadanie chýb atď. Vhodným príkladom môže byť napríklad online demo nástroja Prodigy [25].



**Zoznam použitej literatúry**

[1]        D. C. Brabham, _Crowdsourcing_. 2013.

[2]        K. J. Stol, B. Caglayan, and B. Fitzgerald, &quot;Competition-Based Crowdsourcing Software Development: A Multi-Method Study from a Customer Perspective,&quot; _IEEE Trans. Softw. Eng._, vol. 45, no. 3, pp. 237–260, 2019.

[3]        C. Callison-Burch, L. Ungar, and E. Pavlick, &quot;Crowdsourcing for NLP,&quot; no. January, pp. 2–3, 2015.

[4]        T. Buecheler, J. H. Sieg, R. M. Füchslin, and R. Pfeifer, &quot;Crowdsourcing, open innovation and collective intelligence in the scientific method: A research agenda and operational framework,&quot; _Artif. Life XII Proc. 12th Int. Conf. Synth. Simul. Living Syst. ALIFE 2010_, no. August, pp. 679–686, 2010.

[5]        B. A. Huberman, D. M. Romero, and F. Wu, &quot;Crowdsourcing, attention and productivity,&quot; _J. Inf. Sci._, vol. 35, no. 6, pp. 758–765, 2009.

[6]        F. Kleemann, G. G. Voß, and K. Rieder, &quot;Un(der)paid Innovators: The Commercial Utiliza-tion of Consumer Work through Crowdsourcing,&quot; _Sci. Technol. Innov. Stud._, vol. 4, no. 1, p. PP. 5-26, 2008.

[7]        E. Estellés-Arolas and F. González-Ladrón-De-Guevara, &quot;Towards an integrated crowdsourcing definition,&quot; _J. Inf. Sci._, vol. 38, no. 2, pp. 189–200, 2012.

[8]        E. Schenk and C. Guittard, &quot;Towards a characterization of crowdsourcing practices,&quot; _J. Innov. Econ._, vol. 7, no. 1, p. 93, 2011.

[9]        K. R. Lakhani and J. A. Panetta, &quot;The Principles of Distributed Innovation.&quot;

[10]        H. Hobson, Lane; Howard, Cole; Hapke, _Natural Language Processing in Action_. Manning, 2019.

[11]        &quot;The essential guide to how NLP works - Riti Dass - Medium.&quot; [Online]. Available: https://medium.com/@ritidass29/the-essential-guide-to-how-nlp-works-4d3bb23faf76. [Accessed: 11-Nov-2019].

[12]        &quot;ACE 2004 Multilingual Training Corpus - Linguistic Data Consortium.&quot; [Online]. Available: https://catalog.ldc.upenn.edu/LDC2005T09. [Accessed: 12-Nov-2019].

[13]        &quot;Text Analysis Conference (TAC).&quot; [Online]. Available: https://tac.nist.gov/. [Accessed: 12-Nov-2019].

[14]        &quot;SENSEVAL 3 – evaluation exercises.&quot; [Online]. Available: https://www.senseval.org/. [Accessed: 12-Nov-2019].

[15]        &quot;OntoNotes Release 5.0 - Linguistic Data Consortium.&quot; [Online]. Available: https://catalog.ldc.upenn.edu/LDC2013T19. [Accessed: 12-Nov-2019].

[16]        M. Sabou, K. Bontcheva, L. Derczynski, and A. Scharl, &quot;Corpus annotation through crowdsourcing: Towards best practice guidelines,&quot; _Proc. 9th Int. Conf. Lang. Resour. Eval. Lr. 2014_, no. May 2018, pp. 859–866, 2014.

[17]        R. Snow, B. O&#39;connor, D. Jurafsky, and A. Y. Ng, &quot;Cheap and Fast-But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks,&quot; 2008.

[18]        Q. Su, D. Pavlov, J.-H. Chow, and W. C. Baker, _Internet-Scale Collection of Human-Reviewed Data_. .

[19]        B. Mellebeek, F. Benavent, J. Grivolla, J. Codina, M. R. Costa-Jussà, and R. Banchs, &quot;Opinion Mining of Spanish Customer Comments with Non-Expert Annotations on Mechanical Turk.&quot;

[20]        G. Parent and M. Eskenazi, &quot;Clustering dictionary definitions using Amazon Mechanical Turk.&quot;

[21]        M. Heilman and N. A. Smith, &quot;Rating Computer-Generated Questions with Mechanical Turk,&quot; Association for Computational Linguistics, 2010.

[22]        T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Martineau, and M. Dredze, &quot;Annotating Named Entities in Twitter Data with Crowdsourcing,&quot; Association for Computational Linguistics, 2010.

[23]        N. Lawson, K. Eustice, M. Perkowitz, and M. Yetisgen-Yildiz, &quot;Annotating Large Email Datasets for Named Entity Recognition with Mechanical Turk.&quot;

[24]        D. Jurgens and R. Navigli, &quot;It&#39;s All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation.&quot;

[25]        &quot;Live Demo · Prodigy · An annotation tool for AI, Machine Learning &amp; NLP.&quot; [Online]. Available: https://prodi.gy/demo?view\_id=ner. [Accessed: 12-Nov-2019].
